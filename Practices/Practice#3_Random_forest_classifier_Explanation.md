# Practice #3  Random forest classifier Explanation

This practice is straightforward. Decision trees are a popular family of classification and regression methods. You can find more information about the spark.ml implementation in the section on decision trees.

The following examples load a dataset in LibSVM format, divide it into test and training sets, train on the first dataset, and then evaluate on the retained test set. We use two feature transformers to prepare the data; 

***These index categories help the label and categorical features, adding metadata to the DataFrame that the decision tree algorithm can recognize.***

> Random forest (or random forests)  is a combination of predictor trees
> such that each tree depends on the values ​​of a random vector tested
> independently and with the same distribution for each of these.

It is a substantial modification of bagging that builds a long collection of uncorrelated trees and then averages them. Selecting a random subset of attributes is an example of the random subspace method, which, according to the formulation, is one way to carry out stochastic discrimination.
In many problems the performance of the random forest algorithm is very similar to that of boosting, and it is simpler to train and adjust. 

## *As a consequence, the Random forest is popular and widely used.*
